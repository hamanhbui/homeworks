# Ha Bui
# hbui13@jhu.edu

# Inputs:
#   None
# Output:
#   The pvalue of the presence of a causal effect of the binary treatment
#   according to the nonparametric permutation test.
permutation_pvalue = function(){

    vals <- c(1, 0, 1, 0, 1, 0, 1, 70, 65, 88, 69, 89, 73, 92)
    m <- matrix(vals, ncol=2)
    
    # your code here
    #Neyman
    k = 0
    E_1 = 0
    E_2 = 0
  
    for(row in 1:nrow(m)) {
        E_1 = E_1 + (m[row,2] * m[row,1])
        E_2 = E_2 + (m[row,2] * (1 - m[row,1]))
        if(m[row,1] == 1){
            k = k + 1
        }
    }

    ACE_nm = ((1/k) * E_1) - ((1/(nrow(m)-k)) * E_2)

    #Fisher
    N = 0
    pers = gtools::permutations(7, 3, m[,2])
    for(row_p in 1:nrow(pers)){
        k = 0
        E_1 = 0
        E_2 = 0
        for(row_m in 1:nrow(m)){
            a_tmp = 1
            for(col_p in 1:ncol(pers)){
                if(m[row_m,2] == pers[row_p,col_p]){
                  a_tmp = 0
                }
            }
            E_1 = E_1 + (m[row_m,2] * a_tmp)
            E_2 = E_2 + (m[row_m,2] * (1 - a_tmp))
            if (a_tmp == 1){
                k = k + 1
            }
        }

        ACE_per = ((1/k) * E_1) - ((1/(nrow(m)-k)) * E_2)
        if(ACE_per > ACE_nm){
            N = N + 1
        }
    }
    pval = N/nrow(pers)

    return(pval)

}

df_make = function(mat){
    df <- as.data.frame(mat)
    colnames(df) <- paste("x", c(1:dim(mat)[2]), sep="")
    return(df)
}

df_resample = function(df){
    nrow <- dim(df)[1]
    ncol <- dim(df)[2]
    sampled_df <- df_make(matrix(nrow=nrow, ncol=ncol))

    sampled_rows <- sample(c(1:nrow), nrow, replace=TRUE)
    for (i in c(1:nrow)) {
        sampled_df[i, ] <- df[sampled_rows[i],]
    }
    
    return(sampled_df)
}

bootstrap_ci = function(df, k, f, q){
    
    est <- f(df)
    
    mu <- rep(0, k)
    for(i in 1:k){
        df_r <- df_resample(df)
        
        mu[i] <- f(df_r)
    }
    mu <- sort(mu)
    lower <-  est + quantile(est - mu, q, names = FALSE)
    upper <- est + quantile(est - mu, 1-q, names = FALSE)

    c(est, lower, upper, upper - lower)
}

logisprob = function(x, w){
    
    logis(x %*% w)
}

logis = function(x){
    
    1 / (1 + exp(-x))
}

# Input:
#   a training dataset given as a set of feature rows, represented
#   by a n by k matrix X, and a set of corresponding
#   output predictions, represented by a n by 1 matrix y.
# Output:
#   A row vector of weights for a logistic regression model (with no intercept)
#   maximizing the likelihood of observing the data.
logisreg = function(X, y){
    
    w <- rep(0, dim(X)[2])
    repeat {
        w0 <- w
        p <- apply(X, 1, function(row) logisprob(row, w))
        W <- diag(p * (1 - p))
        w <- w + solve(t(X) %*% W %*% X) %*% t(X) %*% (y - p)
        if(abs(sum(w0 - w)) < 0.001){
            break;
        }
    }
    t(w)
}

# Input:
#   a training dataset given as a set of feature rows, represented
#   by a n by k matrix X, and a set of corresponding
#   output predictions, represented by a n by 1 matrix y.
# Output:
#   A row vector of weights for a linear regression model (with an intercept)
#   maximizing the likelihood of observing the data.
linreg = function(X, y){
    X <- cbind(matrix(c(1), nrow=dim(X)[1], ncol=1), X)
    A <- solve(t(X) %*% X) %*% t(X) %*% y
    return(A)
}

# Input:
#   df : a data frame representing a data set
#   eY_model: a function (closure) that takes in X, y and returns weights
#             for a linear model of y given X
#             (for example, good_linreg or bad_linreg)
# Output:
#   The average causal effect (ACE), assuming the data
#   was generated by a conditionally ignorable causal model, where
#   for any value a, Y(a) is independent of A conditional on C.
#   A values lie in the 1st column of the data frame, Y values lie in
#   the 12th column of the data frame, and C value lie in 2nd to
#   11th columns (inclusive) of the data frame.
#   Estimate the ACE using the parametric g-formula, using the
#   linear model for E[Y | A, X] given by eY_model
gformula = function(df, eY_model){
    m <- as.matrix(df)
    y <- m[,ncol(m), drop=FALSE]
    X <- m[,3:12]
    W <- eY_model(X, y)

    # your code here
    A = X[,1]
    N = dim(X)[1]

    A_0 = rep(0, length(A))
    A_1 = rep(1, length(A))
    
    X_0 = X
    X_0[,1] = A_0
    X_1 = X
    X_1[,1] = A_1

    y_hat_0 = W[1] + X_0 %*% W[2:11]
    y_hat_1 = W[1] + X_1 %*% W[2:11]

    E_1 = 0
    E_2 = 0

    for(i in 1:NROW(X)){
        E_1 = E_1 + y_hat_1[i]
        E_2 = E_2 + y_hat_0[i]
    }

    ace = (1/N) * (E_1 - E_2)

    return(ace)
}

# Input:
#   df : a data frame representing a data set
#   pA_model: a function (closure) that takes in X, A and returns weights
#             for a logistic model of A given X
#             (for example, good_logisreg or bad_logisreg)
# Output:
#   The average causal effect, assuming the data
#   was generated by a conditionally ignorable causal model, where
#   for any value a, Y(a) is independent of A conditional on C.
#   A values lie in the 1st column of the data frame, Y values lie in
#   the 12th column of the data frame, and C value lie in 2nd to
#   11th columns (inclusive) of the data frame.
#   Estimate the ACE using inverse probability weighting, using the
#   logistic model P(A | X), given in pA_model.
ipw = function(df, pA_model){
    m <- as.matrix(df)

    A <- m[, 3, drop=FALSE]
    X <- m[, 4:12]
    y <- m[, 13, drop=FALSE]
    N <- dim(X)[1]

    W <- pA_model(X, A)

    # your code here
    W = as.vector(W)
    # a_hat = X %*% W
    a_hat = 1 / (1 + exp(-(X %*% W)))
    E_1 = 0
    E_2 = 0
    
    for(i in 1:NROW(X)){
        E_1 = E_1 + ((y[i] * A[i]) / a_hat[i])
        E_2 = E_2 + ((y[i] * (1 - A[i])) / (1 - a_hat[i]))
    }

    ace = (1/N) * (E_1 - E_2)

    return(ace)
}


# Input:
#   df : a data frame representing a data set
#   pA_model: a function (closure) that takes in X, A and returns weights
#             for a logistic model of A given X
#             (for example, good_logisreg or bad_logisreg)
#   eY_model: a function (closure) that takes in X, y and returns weights
#             for a linear model of y given X
#             (for example, good_linreg or bad_linreg)
# Output:
#   The average causal effect, assuming the data
#   was generated by a conditionally ignorable causal model, where
#   for any value a, Y(a) is independent of A conditional on C.
#   A values lie in the 1st column of the data frame, Y values lie in
#   the 12th column of the data frame, and C value lie in 2nd to
#   11th columns (inclusive) of the data frame.
#   Estimate the ACE using augmented inverse probability weighted,
#   with a logistic regression treatment model (as given in pA_model)
#   and a linear regression outcome model (as given in eY_model).
aipw = function(df, pA_model, eY_model){
    m <- as.matrix(df)

    A <- m[, 3, drop=FALSE]
    X <- m[, 4:12]
    y <- m[, 13, drop=FALSE]
    N <- dim(X)[1]

    W1 <- pA_model(X, A)
    W2 <- eY_model(cbind(A, X), y)
    
    # your code here
    W1 = as.vector(W1)
    # a_hat = X %*% W1
    a_hat = 1 / (1 + exp(-(X %*% W1)))

    A_0 = rep(0, length(A))
    A_1 = rep(1, length(A))
    
    X_0 = cbind(A_0, X) 
    X_1 = cbind(A_1, X)

    y_hat_0 = W2[1] + X_0 %*% W2[2:11]
    y_hat_1 = W2[1] + X_1 %*% W2[2:11]

    E_1 = 0
    E_2 = 0
    
    for(i in 1:NROW(X)){
        E_1 = E_1 + ((((y[i] - y_hat_1[i]) * A[i]) / a_hat[i]) + y_hat_1[i])
        E_2 = E_2 + ((((y[i] - y_hat_0[i]) * (1 - A[i])) / (1 - a_hat[i])) + y_hat_0[i])
    }

    ace = (1/N) * (E_1 - E_2)

    return(ace)

}

main = function() {
    set.seed(0)
    df <- read.csv("ihdp_subset2.csv", header = TRUE)
    k <- 100

    good_logis = function(X, A) {
        logisreg(X, A)
    }
    bad_logis = function(X, A) {
        c(logisreg(X[,1:4], A), 0, 0, 0, 0, 0)
    }
    good_lin = function(X, y) {
        linreg(X, y)
    }
    bad_lin = function(X, y) {
        c(linreg(X[,1:5], y), 0, 0, 0, 0, 0)
    }

    print(permutation_pvalue())
    print(bootstrap_ci(df, k, f=function(df) aipw(df, good_logis, good_lin), q = 0.025))
    print(bootstrap_ci(df, k, f=function(df) aipw(df, bad_logis, good_lin), q = 0.025))
    print(bootstrap_ci(df, k, f=function(df) aipw(df, good_logis, bad_lin), q = 0.025))
    print(bootstrap_ci(df, k, f=function(df) aipw(df, bad_logis, bad_lin), q = 0.025))
    print(bootstrap_ci(df, k, f=function(df) gformula(df, good_lin), q = 0.025))
    print(bootstrap_ci(df, k, f=function(df) gformula(df, bad_lin), q = 0.025))
    print(bootstrap_ci(df, k, f=function(df) ipw(df, good_logis), q = 0.025))
    print(bootstrap_ci(df, k, f=function(df) ipw(df, bad_logis), q = 0.025))
}

main()
